1. 인공 신경망
1) 입력층 >> 출력층
	(밀집층, dense layer: 입력층과 출력층을 연결하는 각각의 계산식)
	* 입력층(input layer): 데이터
	* 출력층(output layer): 결과
	* 뉴런(neuron, 유닛, unit): 각각 출력층의 객체
* 완전 연결층(fully connected layer):
	입력층과 출력층이 빠짐없이 연결된 형태

2) 검증
* 교차 검증이 아닌 별도의 검증 세트를 준비
- 딥러닝의 데이터 셋은 대부분 충분 >> 검증 점수가 안정적
- 교차 검증을 수행하기에 훈련 시간이 오래 걸림

3) 활성화 함수(activation function)
 뉴런의 선형 방정식 계산 결과에 적용되는 함수
	ex) softmax(다중분류), sigmoid(이진분류) 등

4) 원-핫 인코딩(one-hot encoding):
	맞는 타겟값의 클래스만 1, 나머지는 0인 배열로 변경


2. 심층 신경망(deep neural network, DNN)
1) 은닉층(hidden layer): 입력층과 출력층 사이의 모든 층
	단순한 입력 > 출력의 선형 모델을 비선형으로 변환하며
		성능, 복잡한 패턴을 분석 가능하게 함
	은닉층은 출력층보다 많아야 정보가 부족하지 않게 전달됨

2) 렐루 함수(ReLU): 시그모이드 함수는 수가 양극으로 갈수록 세밀하지 못함
	>> 음수 = 0, 양수 > 선형 함수
	>> max(0, z)라고 함: z가 0보다 크면 z, z가 0보다 작으면 0을 반환

3) flatten 층: 입력 차원을 1D로 펼치는 역할

4) 옵티마이저(optimizer): 다양한 종류의 경사 하강법 알고리즘
 (1) RMSprop: 케라스의 기본 경사 하강법
 (2) SGD: 확률적 경사 하강법
	- 모멘텀 최적화(momoentum optimization):
	- 네스테로프 모멘텀 최적화(nesterov momentum optimization):
		모멘텀 최적화를 2번 반복
 (3) Adagrad: 적응적 학습률 옵티마이저
	* 적응적 학습률(adaptive learning rate):
		모델이 최적점에 가까이 갈수록 학습률을 낮춰 안정적인 수렴
 (4) Adam: 모멘텀 최적화 & 적응적 학습률 적용

5) 순전파 vs 역전파
 (1) 순전파(forward propagation, 정방향 계산, forward pass):
	입력층 >> (은닉층) >> 출력층
 (2) 역전파(backpropagation):
	출력층 >> (은닉층) >> 입력층


3. 신경망 모델 훈련
1) 머신러닝 vs 딥러닝
- 머신러닝: 매개변수 조정, 모델 구조 고정
- 딥러닝: 뉴런 수, 활성화 함수 등 모델 구조 조정

2) 손실 곡선
- 인공 신경망 모델은 정확도가 아닌 손실 감소에 비례하여 성능이 올라감

3) 드롭아웃(dropout): 일부 뉴런을 랜덤하게 꺼 과대적합을 막음

4) 콜백(callback): 훈련 중간에 특정 작업을 수행
 (1) Modelcheckpoint callback: 에포크마다 모델을 저장